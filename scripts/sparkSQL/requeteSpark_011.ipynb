{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Requêtes Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des librairies\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de spark\n",
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.config(\"spark.sql.broadcastTimeout\", \"36000\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du mot à chercher\n",
    "mot_cle = 'cinéma surréaliste'.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Création des dataframes à partir des fichiers avro: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- p_title: string (nullable = true)\n",
      " |-- p_namespace: long (nullable = true)\n",
      " |-- p_id: long (nullable = true)\n",
      " |-- p_revisions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- r_id: long (nullable = true)\n",
      " |    |    |-- r_parent_id: long (nullable = true)\n",
      " |    |    |-- r_timestamp: string (nullable = true)\n",
      " |    |    |-- r_contributor: struct (nullable = true)\n",
      " |    |    |    |-- r_username: string (nullable = true)\n",
      " |    |    |    |-- r_contributor_id: long (nullable = true)\n",
      " |    |    |    |-- r_contributor_ip: string (nullable = true)\n",
      " |    |    |-- r_minor: string (nullable = true)\n",
      " |    |    |-- r_comment: string (nullable = true)\n",
      " |    |    |-- r_model: string (nullable = true)\n",
      " |    |    |-- r_format: string (nullable = true)\n",
      " |    |    |-- r_text: struct (nullable = true)\n",
      " |    |    |    |-- r_text_id: long (nullable = true)\n",
      " |    |    |    |-- r_text_bytes: long (nullable = true)\n",
      " |    |    |-- r_sha1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données sur history contribition\n",
    "path_contribHistory = \"hdfs://localhost:8000/data/frwiki/frwiki-20200201/master/full/frwiki-20200201-stub-meta-history-*\"\n",
    "df_contribHistory = spark.read.format(\"avro\").load(path_contribHistory)\n",
    "df_contribHistory.printSchema()\n",
    "df_contribHistory.createOrReplaceTempView(\"pagesHistory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pl_from: long (nullable = true)\n",
      " |-- pl_namespace: long (nullable = true)\n",
      " |-- pl_title: string (nullable = true)\n",
      " |-- pl_from_namespace: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger les données sur les liens des pages\n",
    "path_pagelink = 'hdfs://localhost:8000/data/frwiki/frwiki-20200201/master/full/frwiki-20200201-pagelinks-*'\n",
    "df_pl = spark.read.format(\"avro\").load(path_pagelink)\n",
    "df_pl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: long (nullable = true)\n",
      " |-- dest_title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enlever les colonnes qu'on n'aura pas besoins et renommer celles à garder pour nous reperer\n",
    "df_pl2 = df_pl.drop(\"pl_namespace\").drop(\"pl_from_namespace\")\\\n",
    "            .withColumnRenamed(\"pl_from\", \"src\")\\\n",
    "            .withColumnRenamed(\"pl_title\", \"dest_title\")\n",
    "df_pl2.printSchema()\n",
    "df_pl2.createOrReplaceTempView(\"pageslink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Requêtes filtrages des dataframes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.1. Filtrer les titres qui correspondent à notre sujet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|   p_id|             p_title|         p_revisions|\n",
      "+-------+--------------------+--------------------+\n",
      "|2785024|  Cinéma surréaliste|[[26419526,, 2008...|\n",
      "|4394563|Discussion:Cinéma...|[[48947910,, 2010...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recherche du mot-clé dans le titre de history\n",
    "premier_df = spark.sql(\"\"\"\n",
    "    SELECT p_id, p_title, p_revisions FROM pagesHistory WHERE LOWER(p_title) LIKE '%{}%'\n",
    "    \"\"\".format(mot_cle))\n",
    "premier_df.persist()\n",
    "premier_df.createOrReplaceTempView(\"premier_result\")\n",
    "premier_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2. Faire les liens sortants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------------+\n",
      "|   p_id|     p_title|         p_revisions|\n",
      "+-------+------------+--------------------+\n",
      "| 123579|      Fluxus|[[888563,, 2004-0...|\n",
      "| 876757|      Arzach|[[8209060,, 2006-...|\n",
      "|   1030|  Eraserhead|[[2289,, 2002-06-...|\n",
      "|4700547|      Rubber|[[53061781,, 2010...|\n",
      "|4094875|  Doppelherz|[[44929883,, 2009...|\n",
      "| 799539|       Freud|[[7302075,, 2006-...|\n",
      "|  42242|Psychanalyse|[[206063,, 2003-1...|\n",
      "|1305148|     Destino|[[13714307,, 2007...|\n",
      "| 746810|   Lettriste|[[6752181,, 2006-...|\n",
      "|  64290|  Taxidermie|[[344808,, 2004-0...|\n",
      "|3216891|     Accueil|[[32735716,, 2008...|\n",
      "| 307239|      Aaltra|[[2594075,, 2005-...|\n",
      "+-------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Les liens sortants de notre page\n",
    "deuxieme_df = spark.sql(\"\"\"\n",
    "    SELECT p_id, p_title, p_revisions FROM pagesHistory ph INNER JOIN\n",
    "    (\n",
    "        SELECT pageslink.dest_title FROM premier_result INNER JOIN pageslink \n",
    "        ON (premier_result.p_id = pageslink.src)\n",
    "    ) pUsingOurLink ON (ph.p_title = pUsingOurLink.dest_title)\n",
    "    \"\"\")\n",
    "deuxieme_df.persist()\n",
    "deuxieme_df.createOrReplaceTempView(\"deuxieme_result\")\n",
    "deuxieme_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.3. Faire les liens entrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----------+\n",
      "|p_id|p_title|p_revisions|\n",
      "+----+-------+-----------+\n",
      "+----+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Les liens entrants dans notre page\n",
    "troisieme_df = spark.sql(\"\"\"\n",
    "    SELECT p_id, p_title, p_revisions FROM pagesHistory ph \n",
    "    INNER JOIN \n",
    "    (\n",
    "        SELECT pageslink.src FROM pageslink\n",
    "        WHERE LOWER(dest_title) LIKE '%{}%'\n",
    "    ) linkThatOurPageUse \n",
    "    ON (ph.p_id = linkThatOurPageUse.src)\n",
    "    \"\"\".format(mot_cle))\n",
    "troisieme_df.persist()\n",
    "troisieme_df.createOrReplaceTempView(\"troisieme_result\")\n",
    "troisieme_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Union des trois dataframes obtenues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       r_contributor|\n",
      "+--------------------+\n",
      "|[[ARoublev68, 358...|\n",
      "|[[Bub's wikibot, ...|\n",
      "|[[,, 65.92.107.10...|\n",
      "|[[Kostia, 39699,]...|\n",
      "|[[Shaihulud, 4,],...|\n",
      "|[[,, 81.67.26.83]...|\n",
      "|[[Thekeuponsauvag...|\n",
      "|[[16@r, 40933,], ...|\n",
      "|[[Charlie brown, ...|\n",
      "|[[,, 86.198.68.12...|\n",
      "|[[,, 86.71.191.150]]|\n",
      "|[[Somniman, 3066,...|\n",
      "|[[VIGNERON, 3942,...|\n",
      "|[[Gadro, 16433,],...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union verticale des trois dataframes\n",
    "union_df = spark.sql(\"\"\"\n",
    "        (SELECT p_revisions.r_contributor FROM premier_result)\n",
    "        UNION ALL\n",
    "        (SELECT p_revisions.r_contributor FROM deuxieme_result)\n",
    "        UNION ALL\n",
    "        (SELECT p_revisions.r_contributor FROM troisieme_result)\n",
    "    \"\"\")\n",
    "union_df.persist()\n",
    "union_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Extraction des Contributeurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La fonction explode transforme toues les éléments d'une liste en lignes\n",
    "from pyspark.sql.functions import explode\n",
    "df_exploded = union_df.select(explode(union_df.r_contributor))\n",
    "contributor_df = df_exploded.select(df_exploded.col.r_username, df_exploded.col.r_contributor_id ,df_exploded.col.r_contributor_ip)\n",
    "contributor_df = contributor_df.withColumnRenamed('col.r_username', 'name')\\\n",
    "                                .withColumnRenamed('col.r_contributor_id', 'id')\\\n",
    "                                .withColumnRenamed('col.r_contributor_ip', 'ip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Tri et présentation du résultat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----+-----+\n",
      "|       name|     id|  ip|count|\n",
      "+-----------+-------+----+-----+\n",
      "|     Léon66| 100556|null|  234|\n",
      "|      Jolek| 862133|null|  205|\n",
      "|Pierrette13|1220016|null|  133|\n",
      "+-----------+-------+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contributor_count_df = contributor_df.groupby([\"name\", \"id\", \"ip\"]).count().orderBy(\"count\", ascending=False)\n",
    "contributor_count_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
